{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of optimization images are 44000\n",
      "Number of validation images are 11000\n",
      "Number of testing images are 10000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0917 23:03:23.622506 47439958921344 lazy_loader.py:50] \n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "W0917 23:03:23.624896 47439958921344 deprecation.py:323] From <ipython-input-1-9c799e8e1b3c>:61: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dense instead.\n",
      "W0917 23:03:23.631040 47439958921344 deprecation.py:506] From /util/opt/anaconda/deployed-conda-envs/packages/tensorflow/envs/tensorflow-1.14.0-py36/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W0917 23:03:24.087286 47439958921344 deprecation.py:323] From <ipython-input-1-9c799e8e1b3c>:69: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning calculations...\n",
      "For Epoch 0 The OPTIMIZATION CROSS ENTROPY Is :0.4176049399274317\n",
      "For Epoch 0 The VALIDATION CROSS ENTROPY Is :0.172745247523893\n",
      "\n",
      "For Epoch 1 The OPTIMIZATION CROSS ENTROPY Is :0.2502139800143513\n",
      "For Epoch 1 The VALIDATION CROSS ENTROPY Is :0.14099764555523342\n",
      "\n",
      "For Epoch 2 The OPTIMIZATION CROSS ENTROPY Is :0.22001555049791932\n",
      "For Epoch 2 The VALIDATION CROSS ENTROPY Is :0.12625104618174107\n",
      "\n",
      "For Epoch 3 The OPTIMIZATION CROSS ENTROPY Is :0.2082852795550769\n",
      "For Epoch 3 The VALIDATION CROSS ENTROPY Is :0.11898892547257922\n",
      "\n",
      "For Epoch 4 The OPTIMIZATION CROSS ENTROPY Is :0.20257842850617386\n",
      "For Epoch 4 The VALIDATION CROSS ENTROPY Is :0.1155596684325825\n",
      "\n",
      "For Epoch 5 The OPTIMIZATION CROSS ENTROPY Is :0.19892363402653823\n",
      "For Epoch 5 The VALIDATION CROSS ENTROPY Is :0.11201322635805065\n",
      "\n",
      "For Epoch 6 The OPTIMIZATION CROSS ENTROPY Is :0.19553012927486138\n",
      "For Epoch 6 The VALIDATION CROSS ENTROPY Is :0.1097614425657825\n",
      "\n",
      "For Epoch 7 The OPTIMIZATION CROSS ENTROPY Is :0.19287139460105787\n",
      "For Epoch 7 The VALIDATION CROSS ENTROPY Is :0.10795330998741767\n",
      "\n",
      "For Epoch 8 The OPTIMIZATION CROSS ENTROPY Is :0.1904942293312739\n",
      "For Epoch 8 The VALIDATION CROSS ENTROPY Is :0.10905256572772157\n",
      "\n",
      "For Epoch 9 The OPTIMIZATION CROSS ENTROPY Is :0.18809785905548118\n",
      "For Epoch 9 The VALIDATION CROSS ENTROPY Is :0.10654495207762177\n",
      "\n",
      "For Epoch 10 The OPTIMIZATION CROSS ENTROPY Is :0.18592286318201912\n",
      "For Epoch 10 The VALIDATION CROSS ENTROPY Is :0.1043535576032644\n",
      "\n",
      "For Epoch 11 The OPTIMIZATION CROSS ENTROPY Is :0.18438392293385483\n",
      "For Epoch 11 The VALIDATION CROSS ENTROPY Is :0.10393980113281445\n",
      "\n",
      "For Epoch 12 The OPTIMIZATION CROSS ENTROPY Is :0.18298933037810705\n",
      "For Epoch 12 The VALIDATION CROSS ENTROPY Is :0.10433617030185732\n",
      "\n",
      "For Epoch 13 The OPTIMIZATION CROSS ENTROPY Is :0.18171647600829602\n",
      "For Epoch 13 The VALIDATION CROSS ENTROPY Is :0.10429068044336005\n",
      "\n",
      "For Epoch 14 The OPTIMIZATION CROSS ENTROPY Is :0.18045974566855214\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "tf.reset_default_graph()\n",
    "\n",
    "flags = tf.app.flags\n",
    "flags.DEFINE_string('data_dir', '/work/cse496dl/shared/hackathon/02/mnist/', 'directory where MNIST is located')\n",
    "flags.DEFINE_string('save_dir', 'hackathon_3', 'directory where model graph and weights are saved')\n",
    "flags.DEFINE_integer('batch_size', 50, '')\n",
    "flags.DEFINE_integer('max_epoch_num', 100, '')\n",
    "FLAGS = flags.FLAGS\n",
    "\n",
    "# Used for splitting data into different proportions\n",
    "def split_data(proportion, data, labels):\n",
    "\n",
    "    # Find number of instances\n",
    "    num_examples = data.shape[0]\n",
    "    # Create index to split on\n",
    "    split_idx = int(proportion * num_examples)\n",
    "    \n",
    "    # Split data into validation and optimization pieces\n",
    "    data_1, data_2 = data[:split_idx], data[split_idx:]\n",
    "    # Split labels into validation and optimization pieces\n",
    "    labels_1, labels_2 = labels[:split_idx], labels[split_idx:]\n",
    "    \n",
    "    # Return data as valid_images,valid_labels,opt_images,opt_labels\n",
    "    return data_1, labels_1, data_2, labels_2\n",
    "\n",
    "def main(argv):\n",
    "    # load data\n",
    "    train_images = np.load(FLAGS.data_dir + 'mnist_train_images.npy')\n",
    "    train_labels = np.load(FLAGS.data_dir + 'mnist_train_labels.npy')\n",
    "    test_images = np.load(FLAGS.data_dir + 'mnist_test_images.npy')\n",
    "    test_labels = np.load(FLAGS.data_dir + 'mnist_test_labels.npy')\n",
    "    \n",
    "    # Split train set into optimization set and validation set\n",
    "    # Shuffle train_images and train_labels before splitting\n",
    "    idx = np.random.permutation(train_images.shape[0])\n",
    "    train_images,train_labels = train_images[idx], train_labels[idx]\n",
    "    valid_images,valid_labels,opt_images,opt_labels = split_data(0.20,train_images,train_labels)\n",
    "    \n",
    "    # Remove original train_images and train_labels from memory\n",
    "    train_images = None\n",
    "    train_labels = None\n",
    "    \n",
    "    # Get number of images in each of the 3 sets\n",
    "    opt_num_examples = opt_images.shape[0]\n",
    "    valid_num_examples = valid_images.shape[0]\n",
    "    test_num_examples = test_images.shape[0]\n",
    "    \n",
    "    print('Number of optimization images are %i' %opt_num_examples)\n",
    "    print('Number of validation images are %i' %valid_num_examples)\n",
    "    print('Number of testing images are %i' %test_num_examples)\n",
    "    print('')\n",
    "    \n",
    "    # specify the network\n",
    "    x = tf.placeholder(tf.float32, [None, 784], name='data')\n",
    "    y = tf.placeholder(tf.float32, [None, 10], name='label')\n",
    "    with tf.name_scope('linear_model') as scope:\n",
    "        # L2 regularization has been added to this layer\n",
    "        hidden = tf.layers.dense(x, 400, activation=tf.nn.relu, name='hidden_layer',\n",
    "                                kernel_regularizer=tf.contrib.layers.l2_regularizer(scale=0.01))\n",
    "        # L2 regularization has been added to this layer\n",
    "        output = tf.layers.dense(hidden, 10, name='output_layer',\n",
    "                                kernel_regularizer=tf.contrib.layers.l2_regularizer(scale=0.01))\n",
    "        tf.identity(output, name='model_output')\n",
    "\n",
    "    # define classification loss\n",
    "    cross_entropy  = tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=output)\n",
    "    \n",
    "    # collect the regularization losses\n",
    "    regularization_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n",
    "    # this value is what we'll pass to `minimize`\n",
    "    xentropy_w_reg = cross_entropy + 0.1 * sum(regularization_losses)\n",
    "    \n",
    "    confusion_matrix_op = tf.confusion_matrix(tf.argmax(y, axis=1), tf.argmax(output, axis=1), num_classes=10)\n",
    "\n",
    "\n",
    "    # set up training and saving functionality\n",
    "    global_step_tensor = tf.get_variable('global_step', trainable=False, shape=[], initializer=tf.zeros_initializer)\n",
    "    optimizer = tf.train.AdamOptimizer()\n",
    "    train_op = optimizer.minimize(xentropy_w_reg, global_step=global_step_tensor)\n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "    with tf.Session() as session:\n",
    "        session.run(tf.global_variables_initializer())\n",
    "        \n",
    "        # Location to save sessions\n",
    "        save_directory = './hackathon3_sessions'\n",
    "        \n",
    "        # For early stopping\n",
    "        best_cost=10**20 # Initialize so immediately replaced\n",
    "        early_stop_epoch_num = 5 # Number of epochs to see no improvement before deciding to stop\n",
    "\n",
    "        # run training\n",
    "        batch_size = FLAGS.batch_size\n",
    "        ########################################################################\n",
    "        print('Beginning calculations...')\n",
    "        for epoch in range(FLAGS.max_epoch_num):\n",
    "    \n",
    "            #### Go through optimization images for this epoch ####\n",
    "            # run gradient steps and report mean loss on train data\n",
    "            ce_vals = []\n",
    "            tot_opt_minibatch = opt_num_examples // batch_size\n",
    "            for i in range(tot_opt_minibatch):\n",
    "                batch_xs = opt_images[i*batch_size:(i+1)*batch_size, :]\n",
    "                batch_ys = opt_labels[i*batch_size:(i+1)*batch_size, :]       \n",
    "                _, opt_ce = session.run([train_op, tf.reduce_mean(xentropy_w_reg)], {x: batch_xs, y: batch_ys})\n",
    "                ce_vals.append(opt_ce)\n",
    "            avg_opt_ce = sum(ce_vals) / len(ce_vals)\n",
    "            print('For Epoch %i The OPTIMIZATION CROSS ENTROPY Is :' %epoch + str(avg_opt_ce))\n",
    "            #############################\n",
    "\n",
    "            #### Go through validation images for this epoch ####\n",
    "            # Calculate validation loss\n",
    "            ce_vals = []\n",
    "            for i in range(valid_num_examples // batch_size):\n",
    "                batch_xs = valid_images[i*batch_size:(i+1)*batch_size, :]\n",
    "                batch_ys = valid_labels[i*batch_size:(i+1)*batch_size, :]\n",
    "                valid_ce, _ = session.run([tf.reduce_mean(cross_entropy), confusion_matrix_op], {x: batch_xs, y: batch_ys})\n",
    "                ce_vals.append(valid_ce)\n",
    "            avg_valid_ce = sum(ce_vals) / len(ce_vals)\n",
    "            print('For Epoch %i The VALIDATION CROSS ENTROPY Is :' %epoch + str(avg_valid_ce))\n",
    "            print('')\n",
    "            ##############################\n",
    "            \n",
    "            #### Initiate early stopping check/update ####\n",
    "            if avg_valid_ce < best_cost:\n",
    "                best_epoch = epoch\n",
    "                # Save session\n",
    "                saved_session = saver.save(session, os.path.join(save_directory, \"fashion_mnist_classification\"))\n",
    "                best_cost = avg_valid_ce\n",
    "                last_improvement = 0\n",
    "            else:\n",
    "                last_improvement += 1\n",
    "                \n",
    "            # Checks if we have not seen improvement for required number of epochs\n",
    "            if last_improvement >= early_stop_epoch_num:\n",
    "                print(\"No improvement found during last iterations, stopping optimization...\")\n",
    "                saver.restore(session, saved_session) # restore session with the best cost\n",
    "                # Break out from the loop.\n",
    "                break\n",
    "            #####################################\n",
    "        ########################################################################\n",
    "                \n",
    "        # After breaking out of loop assess model using the set aside test set\n",
    "        \n",
    "        # Calculate test loss\n",
    "        print('Processing test set')\n",
    "        ce_vals = []\n",
    "        conf_mxs = []\n",
    "        for i in range(test_num_examples // batch_size):\n",
    "            batch_xs = test_images[i*batch_size:(i+1)*batch_size, :]\n",
    "            batch_ys = test_labels[i*batch_size:(i+1)*batch_size, :]\n",
    "            test_ce, conf_matrix = session.run([tf.reduce_mean(xentropy_w_reg), confusion_matrix_op], {x: batch_xs, y: batch_ys})\n",
    "            ce_vals.append(test_ce)\n",
    "            conf_mxs.append(conf_matrix)\n",
    "        avg_test_ce = sum(ce_vals) / len(ce_vals)\n",
    "        print('TEST CROSS ENTROPY: ' + str(avg_test_ce))\n",
    "        print('TEST CONFUSION MATRIX:')\n",
    "        print(str(sum(conf_mxs)))\n",
    "        print('')\n",
    "        print('The epoch we stopped at was %i' %best_epoch)\n",
    "        \n",
    "        # Save session at end before final exit\n",
    "        path_prefix = saver.save(session, os.path.join(save_directory, \"fashion_mnist_classification\"))\n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "    tf.app.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorFlow 1.14 (py36)",
   "language": "python",
   "name": "tensorflow-1.14-py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
