{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hackathon #2\n",
    "\n",
    "Topics: \n",
    "- Dense layers\n",
    "- Gradient descent optimization\n",
    "- Training by minibatch/gradient step and epoch\n",
    "- TensorBoard\n",
    "\n",
    "This is all setup in a IPython notebook so you can run any code you want to experiment with. Feel free to edit any cell, or add some to run your own code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We'll start with our library imports...\n",
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow as tf         # to specify and run computation graphs\n",
    "import numpy as np              # for numerical operations taking place outside of the TF graph\n",
    "import matplotlib.pyplot as plt # to draw plots\n",
    "\n",
    "mnist_dir = '/work/cse496dl/shared/hackathon/02/mnist/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A First Attempt at Classifying MNIST\n",
    "\n",
    "MNIST is a dataset of greyscale `28x28` handwritten digits labelled 1 through 9. We'll use it for a 10-class problem to learn the basics of classification.\n",
    "\n",
    "Let's have a look at the data first. We load the data from `numpy` save files using `np.load` and can visualize it with matplotlib's `plt.imshow`. The images (shape `[28,28]`) are flat when we first load them (shape `[784]`), so we'll have to [np.reshape](https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.reshape.html) any images we want to visualize. The labels are \"one-hot\", or arrays of length equal to the number of classes (in this case, 10) with the `n`-th entry set to 1 and the rest to 0, indicating the integer value `n`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train image tensor shape: (55000, 784)\n",
      "Train label tensor shape: (55000, 10)\n",
      "Test image tensor shape: (10000, 784)\n",
      "Test label tensor shape: (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "# load our dataset, MNIST\n",
    "train_images = np.load(mnist_dir + 'mnist_train_images.npy')\n",
    "print(\"Train image tensor shape: \" + str(train_images.shape))\n",
    "\n",
    "train_labels = np.load(mnist_dir + 'mnist_train_labels.npy')\n",
    "print(\"Train label tensor shape: \" + str(train_labels.shape))\n",
    "\n",
    "test_images = np.load(mnist_dir + 'mnist_test_images.npy')\n",
    "print(\"Test image tensor shape: \" + str(test_images.shape))\n",
    "\n",
    "test_labels = np.load(mnist_dir + 'mnist_test_labels.npy')\n",
    "print(\"Test label tensor shape: \" + str(test_labels.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A label looks like this: [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "And an image looks like this:\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# visualize some of the data\n",
    "idx = np.random.randint(train_images.shape[0])\n",
    "print(\"A label looks like this: \" + str(train_labels[idx]))\n",
    "print(\"And an image looks like this:\")\n",
    "imgplot = plt.imshow(train_images[idx].reshape((28,28)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step to building a simple neural network is to specify layers. The most basic building block is the dense layer (AKA linear layer or fully connected layer), so we'll declare a function that creates the layer. Each dense layer is composed of two variables, the weight matrix `W` and the bias vector `b` as well as a non-linear activation function `a`, to calculate the function `f(x) = a(Wx + b)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dense_layer(x, output_size, activation=tf.identity, name='dense'):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        - x: a rank two tensor, [batch_size, data_size]\n",
    "        - output_size: (int) number of neurons\n",
    "        - activation: non-linear function applied to the output\n",
    "        - name: TensorFlow name scope for variable\n",
    "    Returns:\n",
    "        a rank two tensor with shape [batch_size, output_size]\n",
    "    \"\"\"\n",
    "    \n",
    "    # If we used tf.get_variable it will use existing variable with provided parameters, and create a new one \n",
    "    # only if it does not already exist\n",
    "    \n",
    "    # tf.Variable will always create a new variable\n",
    "    \n",
    "    # Output will be dense/weights:0 (as example output)\n",
    "    with tf.name_scope(name) as scope:\n",
    "        (_, data_size) = x.get_shape().as_list() # Automatically find shape and makes it the correct shape for tensor\n",
    "        W = tf.Variable(tf.truncated_normal([data_size, output_size]), name='weights')\n",
    "        b = tf.Variable(tf.truncated_normal([output_size]), name='bias')\n",
    "        # Runs multiplication + bias through provided activation function\n",
    "        return activation(tf.matmul(x, W) + b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first dimension of the input is reserved to be the \"batch\" dimension, which allows us to run many data through the model simultaneously. The matrix `W` has a row for each input dimension so that each column corresponds to the weights of one linear unit of the layer. After adding the bias vector to the vector resulting from the vector-matrix multiplication, we activate with a non-linearity or the identity function (the latter if we just want to use a linear transformation). [tf.name_scope](https://www.tensorflow.org/api_docs/python/tf/name_scope) is used to group the layer's parameters in TensorFlow's namespace, and its effects can be seen in TensorBoard.\n",
    "\n",
    "TensorFlow variables, which host the model parameters persistently in the graph, are declared with [tf.Variable](https://www.tensorflow.org/api_docs/python/tf/Variable), needing only an initial value to be created. You can also name the variable or set it to be untrainable using optional arguments, but a `Variable` is trainable by default. We use [tf.truncated_normal](https://www.tensorflow.org/api_docs/python/tf/truncated_normal) to provide initial values here to keep it simple, even though there are much better initialization schemes (just make sure you never use a constant, e.g., [tf.zeros](https://www.tensorflow.org/api_docs/python/tf/zeros)).\n",
    "\n",
    "Let's define a simple, two layer network with this function. We activate the first layer with the rectified linear function ([tf.nn.relu](https://www.tensorflow.org/api_docs/python/tf/nn/relu)) and the second layer with [tf.nn.softmax](https://www.tensorflow.org/versions/master/api_docs/python/tf/nn/softmax) so that we can interpret its output as the parameters of a discrete probability distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Placeholder for input layer, None so batch size is variable\n",
    "# Dense layers only handle flat data, images will not work\n",
    "x = tf.placeholder(tf.float32, [None, 784], name='data')\n",
    "\n",
    "# use a single name scope for the model\n",
    "# Names becomes MLPM/dense/weights:0 (for example)\n",
    "with tf.name_scope('multi_layer_perceptron_model') as scope:\n",
    "    hidden = dense_layer(x, 200, activation=tf.nn.relu, name='hidden_layer')\n",
    "    output = dense_layer(hidden, 10, activation=tf.nn.softmax, name='output_layer')\n",
    "    \n",
    "# Useful for visualizing network in tensorboard\n",
    "tf.summary.FileWriter(\"./hackathon2_logs\", tf.get_default_graph()).close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the last line, we log what we just defined with TensorBoard for visualization. We use [tf.get_default_graph](https://www.tensorflow.org/api_docs/python/tf/get_default_graph) to retrieve a handle to the TF graph that we're working in (the default because we haven't specified a particular graph), and then we write a summary of the graph with [tf.summary.FileWriter](https://www.tensorflow.org/api_docs/python/tf/summary/FileWriter). This puts an events file in the \"logs\" directory which maybe opened with TensorBoard. Access by running `tensorboard --logdir=./logs` and pointing a browser at http://localhost:6006. Information on how to do this on Crane can be found on Piazza.\n",
    "\n",
    "Note that, though the number of units in the hidden layer may be chosen freely as a hyperparameter, the number of units in the output layer must equal the number of classes in the classification problem. This allows us to use the output as a categorical distribution over the classes, representing the probability that the input belongs to each class.\n",
    "\n",
    "To summarize how the model is performing in its classification task, let's add a placeholder for the correct output and calculate the [cross-entropy](https://stackoverflow.com/a/41990932) loss between the estimated and correct discrete distributions (i.e., between the model's softmaxed distribution and a distribution with a probability of 1 in the correct class, a one hot vector). We use `EPSILON` to avoid potentially trying to calculate `log(0)`, which is undefined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Second placeholder for labels (holds correct labels)\n",
    "y = tf.placeholder(tf.float32, [None, 10], name='correct_label')\n",
    "\n",
    "# Calculate cross-entropy for calculation\n",
    "# Larger value worse network is doing\n",
    "EPSILON = 1e-10\n",
    "with tf.name_scope('cross_entropy') as scope:\n",
    "    # Epsilon only to avoid evaluating log(0) which of course breaks\n",
    "    cross_entropy = -tf.reduce_sum(output * tf.log(y + EPSILON), axis=1)\n",
    "    \n",
    "tf.summary.FileWriter(\"./hackathon2_logs\", tf.get_default_graph()).close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've constructed the TF graph, we'll create a session, initialize all variables, and run through an epoch of the test set to plot a histogram of the cross-entropy loss. (One epoch is an iteration of minibatches such that each datum is seen once. Minibatches are also frequently referred to as \"batches\", as below)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# calculate values of interest to minibatching and epoch calculation\n",
    "# batch_size is adjustable\n",
    "train_num_examples = train_images.shape[0]\n",
    "test_num_examples = test_images.shape[0]\n",
    "batch_size = 32\n",
    "\n",
    "# finalize the graph\n",
    "with tf.Session() as session:\n",
    "    # Grabs all variables and resets them\n",
    "    session.run(tf.global_variables_initializer())\n",
    "    loss_vals = []\n",
    "    \n",
    "    # Just one epoch\n",
    "    \n",
    "    # Note we did not explicitly call the dense layer we built, but cross-entropy calls output, which calls...\n",
    "    # loop through each test datum once, saving the cross entropy\n",
    "    for i in range(test_num_examples // batch_size):\n",
    "        batch_xs = test_images[i*batch_size:(i+1)*batch_size, :]\n",
    "        batch_ys = test_labels[i*batch_size:(i+1)*batch_size, :]\n",
    "        ce_val = session.run(cross_entropy, {x: batch_xs, y: batch_ys})\n",
    "        loss_vals.append(ce_val)\n",
    "\n",
    "# now plot per-datum losses\n",
    "loss_vals = np.concatenate(loss_vals)\n",
    "hist, bin_edges = np.histogram(loss_vals)\n",
    "plt.bar(bin_edges[:-1], hist, width = 1)\n",
    "plt.xlim(min(bin_edges), max(bin_edges))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "It looks like the model guesses the correct class (corresponding to near-zero loss, the smaller bar on the left) on about 10% of the data, which we would have anticipated for a naive model in a 10 class problem. We can improve this model. Let's try this again, this time training with an optimizer.\n",
    "\n",
    "### A Second Attempt...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0914 14:51:08.800800 47828886352000 deprecation.py:323] From <ipython-input-8-198f5d0b52d2>:9: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dense instead.\n",
      "W0914 14:51:08.807888 47828886352000 deprecation.py:506] From /util/opt/anaconda/deployed-conda-envs/packages/tensorflow/envs/tensorflow-1.14.0-py36/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    }
   ],
   "source": [
    "# Note that above there was no optimizer\n",
    "\n",
    "# Clear the graph so we can re-use it. If this is omitted, we get an error.\n",
    "tf.reset_default_graph()\n",
    "x = tf.placeholder(tf.float32, [None, 784], name='data')\n",
    "# use a single name scope for the model\n",
    "with tf.name_scope('linear_model') as scope:\n",
    "    # Same as above, but now using tensorflow's defined dense layers\n",
    "    hidden = tf.layers.dense(x, 200, activation=tf.nn.relu, name='hidden_layer')\n",
    "    output = tf.layers.dense(hidden, 10, name='output_layer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time, we're using [tf.layers.dense](https://www.tensorflow.org/api_docs/python/tf/layers/dense), which has a very similar API to our custom function, but much more functionality.\n",
    "\n",
    "Further, instead of using a custom cross-entropy function, which has lots of potential for [numerical instability](https://github.com/tensorflow/tensorflow/issues/2462#issuecomment-220842098), we'll use TF's built in function, [tf.nn.softmax_cross_entropy_with_logits_v2](https://www.tensorflow.org/api_docs/python/tf/nn/softmax_cross_entropy_with_logits_v2). It combines the `softmax` activation of the final layer with the cross-entropy calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define classification loss\n",
    "y = tf.placeholder(tf.float32, [None, 10], name='label')\n",
    "# Use built-in function to calculate cross-entroy\n",
    "cross_entropy = tf.nn.softmax_cross_entropy_with_logits_v2(labels=y, logits=output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, using the loss tensor, we'll define an optimizer that uses backpropagation to update the values of each layer's variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# setup optimizer and training operation\n",
    "optimizer = tf.train.AdamOptimizer()\n",
    "train_op = optimizer.minimize(cross_entropy)\n",
    "tf.summary.FileWriter(\"./hackathon2_logs\", tf.get_default_graph()).close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we've chosen the [Adam](https://www.tensorflow.org/api_docs/python/tf/train/AdamOptimizer#minimize) optimizer (usually a safe first choice on any given problem), and use the [minimize](https://www.tensorflow.org/api_docs/python/tf/train/AdamOptimizer#minimize) function which is defined for every TensorFlow optimizer. It returns an operation that automatically calculates the gradient of the provided function, and updates all variables marked trainable. We'll pass it to `sess.run` to train for one epoch and then check the test loss values again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "batch_size = 32\n",
    "session = tf.Session()\n",
    "session.run(tf.global_variables_initializer())\n",
    "\n",
    "# train for one epoch\n",
    "for i in range(train_num_examples // batch_size):\n",
    "    batch_xs = train_images[i*batch_size:(i+1)*batch_size, :]\n",
    "    batch_ys = train_labels[i*batch_size:(i+1)*batch_size, :]       \n",
    "    session.run(train_op, {x: batch_xs, y: batch_ys})\n",
    "\n",
    "# loop through each test datum once\n",
    "loss_vals = []\n",
    "for i in range(test_num_examples // batch_size):\n",
    "    batch_xs = test_images[i*batch_size:(i+1)*batch_size, :]\n",
    "    batch_ys = test_labels[i*batch_size:(i+1)*batch_size, :]\n",
    "    ce_val = session.run(cross_entropy, {x: batch_xs, y: batch_ys})\n",
    "    loss_vals.append(ce_val)\n",
    "\n",
    "# now plot per-datum losses\n",
    "loss_vals = np.concatenate(loss_vals)\n",
    "hist, bin_edges = np.histogram(loss_vals)\n",
    "plt.bar(bin_edges[:-1], hist, width = 1)\n",
    "plt.xlim(min(bin_edges), max(bin_edges))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just one training epoch has dramatically improved the model's performance. But on how many of the instances is it actually guessing the correct label?\n",
    "\n",
    "## Hackathon 2 Exercise 1\n",
    "\n",
    "Write code to calculate the maximum aposteriori (MAP) estimate of the model on the test data, and compare to the true labels to calculate a confusion matrix with [tf.confusion_matrix](https://www.tensorflow.org/api_docs/python/tf/confusion_matrix). (For the inexperienced, [what is a confusion matrix?](https://en.wikipedia.org/wiki/Confusion_matrix))\n",
    "\n",
    "(Hint #0: Re-use and modify my code from above. Try not to reinvent the wheel, but always remember to cite borrowed code.)\n",
    "\n",
    "(Hint #1: The MAP estimate is just the class whose probability is greatest. I reccomend using [tf.argmax](https://www.tensorflow.org/versions/master/api_docs/python/tf/argmax) with the correct `axis` argument to find this to find the max over the correct dimension of the output.)\n",
    "\n",
    "(Hint #2: tf.confusion_matrix is a function that needs be run in a `session.run` call that returns matrices. Store the resulting matrices in a list and then sum to get the matrix for the full test dataset. Remember to specify the `num_classes` argument.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n",
    "# This should only take a few lines of code (hint from hackethon lecture)\n",
    "\n",
    "# Note that above there was no optimizer\n",
    "\n",
    "# Clear the graph so we can re-use it. If this is omitted, we get an error.\n",
    "tf.reset_default_graph()\n",
    "x = tf.placeholder(tf.float32, [None, 784], name='data')\n",
    "# use a single name scope for the model\n",
    "with tf.name_scope('linear_model') as scope:\n",
    "    # Same as above, but now using tensorflow's defined dense layers\n",
    "    hidden = tf.layers.dense(x, 200, activation=tf.nn.relu, name='hidden_layer')\n",
    "    output = tf.layers.dense(hidden, 10, name='output_layer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define classification loss\n",
    "y = tf.placeholder(tf.float32, [None, 10], name='label')\n",
    "# Use built-in function to calculate cross-entroy\n",
    "cross_entropy = tf.nn.softmax_cross_entropy_with_logits_v2(labels=y, logits=output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# setup optimizer and training operation\n",
    "optimizer = tf.train.AdamOptimizer()\n",
    "train_op = optimizer.minimize(cross_entropy)\n",
    "tf.summary.FileWriter(\"./hackathon2_logs\", tf.get_default_graph()).close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "#Using code above we will generate the predicted labels\n",
    "batch_size = 32\n",
    "session = tf.Session()\n",
    "session.run(tf.global_variables_initializer())\n",
    "predicted_class = np.zeros(shape=len(test_labels), dtype=np.int)\n",
    "\n",
    "for i in range(train_num_examples // batch_size):\n",
    "    batch_xs = train_images[i*batch_size:(i+1)*batch_size, :]\n",
    "    batch_ys = train_labels[i*batch_size:(i+1)*batch_size, :]       \n",
    "    session.run(train_op, {x: batch_xs, y: batch_ys})\n",
    "\n",
    "# loop through each test datum once\n",
    "\n",
    "for i in range(test_num_examples // batch_size):\n",
    "    batch_xs = test_images[i*batch_size:(i+1)*batch_size, :]\n",
    "    batch_ys = test_labels[i*batch_size:(i+1)*batch_size, :]\n",
    "    predicted_class[i*batch_size:(i+1)*batch_size] = session.run(tf.argmax(output,1), {x: batch_xs, y: batch_ys})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = np.argmax(test_labels, axis=1)\n",
    "num_classes=10\n",
    "confusion = tf.confusion_matrix(labels=index, predictions=predicted_class, num_classes=num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 970    0    0    1    1    3    1    2    2    0]\n",
      " [   2 1126    2    1    0    1    1    0    2    0]\n",
      " [   9    7  970    7   10    1    3    8   17    0]\n",
      " [   2    1    5  937    3   43    0    6    9    4]\n",
      " [   3    0    5    0  959    0    6    0    3    6]\n",
      " [   4    1    0    3    4  874    2    0    2    2]\n",
      " [  18    4    1    0    9   39  881    0    6    0]\n",
      " [   3   15   13    5    5    0    0  968    3   16]\n",
      " [   4    4    3   14   10   15    1    4  915    4]\n",
      " [   6    8    0    7   24    7    1    5    5  946]]\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "with sess.as_default():\n",
    "    print(sess.run(confusion))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coda\n",
    "\n",
    "### Saving and Loading TF models\n",
    "\n",
    "https://www.tensorflow.org/programmers_guide/saved_model, but we'll also discuss this next time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numpy Broadcasting\n",
    "\n",
    "TensorFlow uses [Numpy broadcasting](https://docs.scipy.org/doc/numpy-1.13.0/user/basics.broadcasting.html) when doing arithmetic with arrays of different shapes.\n",
    "\n",
    "E.g.,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scalar-matrix addition\n",
      "[[2. 2. 2.]\n",
      " [2. 2. 2.]]\n",
      "matrix-vector addition\n",
      "[[1. 2. 3.]\n",
      " [1. 2. 3.]]\n",
      "broadcasting matrix-vector multiplication\n",
      "[[0. 2. 4.]\n",
      " [0. 2. 4.]]\n",
      "proper matrix-vector dot product\n",
      "[6. 6.]\n"
     ]
    }
   ],
   "source": [
    "print(\"scalar-matrix addition\")\n",
    "print(np.ones([2,3]) + 1)\n",
    "print(\"matrix-vector addition\")\n",
    "print(np.ones([2,3]) + np.arange(3))\n",
    "print(\"broadcasting matrix-vector multiplication\")\n",
    "print((np.ones([2,3])*2) * np.arange(3))\n",
    "print(\"proper matrix-vector dot product\")\n",
    "print(np.dot(np.ones([2,3])*2, np.arange(3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The importance of non-linearities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"http://colah.github.io/posts/2014-03-NN-Manifolds-Topology/img/spiral.1-2.2-2-2-2-2-2.gif\">"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "# From Colah's Blog, linearly separating spirals with linear transforms and non-linearities\n",
    "# see http://colah.github.io/posts/2014-03-NN-Manifolds-Topology\n",
    "HTML('<img src=\"http://colah.github.io/posts/2014-03-NN-Manifolds-Topology/img/spiral.1-2.2-2-2-2-2-2.gif\">')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorFlow 1.14 (py36)",
   "language": "python",
   "name": "tensorflow-1.14-py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
