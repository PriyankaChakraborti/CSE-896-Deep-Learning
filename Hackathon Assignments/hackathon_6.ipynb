{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hackathon 6\n",
    "\n",
    "Topics:\n",
    "- Techniques for dimension expansion\n",
    "    - Transpose convolutions\n",
    "    - Sub-pixel convolutions\n",
    "    - ProgressiveGAN upscaling\n",
    "- Autoencoding\n",
    "    - Sparse autoencoders\n",
    "    - De-noising autoencoders\n",
    "\n",
    "This is all setup in a IPython notebook so you can run any code you want to experiment with. Feel free to edit any cell, or add some to run your own code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We'll start with our library imports...\n",
    "from __future__ import print_function\n",
    "\n",
    "import os  # to work with file paths\n",
    "\n",
    "import tensorflow as tf         # to specify and run computation graphs\n",
    "import numpy as np              # for numerical operations taking place outside of the TF graph\n",
    "import matplotlib.pyplot as plt # to draw plots\n",
    "\n",
    "mnist_dir = '/work/cse496dl/shared/hackathon/03/mnist/'\n",
    "cifar_dir = '/work/cse496dl/shared/hackathon/05/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# extract our dataset, MNIST\n",
    "train_data = np.load(mnist_dir + 'mnist_train_images.npy')\n",
    "train_data = np.reshape(train_data, [-1, 28, 28, 1])\n",
    "test_data = np.load(mnist_dir + 'mnist_test_images.npy')\n",
    "test_data = np.reshape(test_data, [-1, 28, 28, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Techniques for dimension expansion\n",
    "\n",
    "Generally we compress high dimensional representations into lower dimensional ones. Now, we're going to study ways of going from lower dimensional to higher. For this, we're going to define a function `upscale_block` in three different ways.\n",
    "\n",
    "#### Transpose convolutions\n",
    "\n",
    "Although we can reverse linear transformations very easily, images are less straightforward to upscale. Aside from naive, classical techniques for upscaling, we can learn to increase the size of images with transpose convolutions. They are sometimes called \"deconvolutions\" because they're the inverse operation of the convolution, but it is actually the transpose (gradient) of conv2d rather than an actual deconvolution. Transpose convolutions are implemented by [tf.layers.conv2d_transpose](https://www.tensorflow.org/api_docs/python/tf/layers/conv2d_transpose)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W1012 12:20:11.221431 47958307618944 deprecation.py:323] From <ipython-input-3-7625a3dc34ca>:6: conv2d_transpose (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.keras.layers.Conv2DTranspose` instead.\n",
      "W1012 12:20:11.240186 47958307618944 deprecation.py:506] From /util/opt/anaconda/deployed-conda-envs/packages/tensorflow/envs/tensorflow-1.14.0-py36/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"conv2d_transpose/Relu:0\", shape=(?, 64, 64, 3), dtype=float32)\n",
      "Parameters: 84\n"
     ]
    }
   ],
   "source": [
    "# Upscales by take summary of image, and try to reconstruct what data was previously there\n",
    "def upscale_block(x, scale=2):\n",
    "    \"\"\" conv2d_transpose \"\"\"\n",
    "    # We scale up dimensions by 2, so 4x4 goes to 8x8\n",
    "    # the 3,3 specifies the number of channels (not just rgb), filter size\n",
    "    return tf.layers.conv2d_transpose(x, 3, 3, strides=(scale, scale), padding='same', activation=tf.nn.relu)\n",
    "\n",
    "# Define graph\n",
    "tf.reset_default_graph()\n",
    "x = tf.placeholder(tf.float32, shape=[None, 32, 32, 3])\n",
    "# Run upscale block on input x\n",
    "up_x = upscale_block(x)\n",
    "print(up_x)\n",
    "\n",
    "# This counts the number of trainable parameters\n",
    "num_params = np.sum([np.prod(v.get_shape().as_list()) for v in tf.trainable_variables()])\n",
    "print('Parameters: ' + str(num_params))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sub-pixel convolutions\n",
    "\n",
    "Another approach is called the sub-pixel convolution, which does a convolution with many channels, and then re-orders the data into the height and width dimensions from the channels dimension:\n",
    "\n",
    "<img src=\"https://ai2-s2-public.s3.amazonaws.com/figures/2017-08-08/03a5b2aac53443e6078f0f63b35d4f95d6d54c5d/2-Figure1-1.png\">\n",
    "\n",
    "(Image sourced from [Real-Time Single Image and Video Super-Resolution Using an Efficient Sub-Pixel Convolutional Neural Network](https://arxiv.org/abs/1609.05158))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1012 12:20:13.022919 47958307618944 deprecation.py:323] From <ipython-input-4-07e0397d2f78>:20: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.keras.layers.Conv2D` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"DepthToSpace:0\", shape=(?, 64, 64, 3), dtype=float32)\n",
      "Parameters: 336\n"
     ]
    }
   ],
   "source": [
    "# This does regular convolution on input to recognize pattersn\n",
    "# Uses many more channels so it can find small details\n",
    "# We interpret each c hannel as pixels of image spread out\n",
    "# Not really understanding how this works...\n",
    "\n",
    "# It is very powerful, but requires many more parameters\n",
    "def upscale_block(x, scale=2):\n",
    "    \"\"\" [Sub-Pixel Convolution](https://arxiv.org/abs/1609.05158) \"\"\"\n",
    "    # c is number of channels\n",
    "    n, w, h, c = x.get_shape().as_list()\n",
    "    # Make sure height and width of image are same after convolution here\n",
    "    # Number of filters is number of channels\n",
    "    # Because we want to double height and width we take 1 channel and turn it into 4\n",
    "    # If input image 32x32 we get out 64,64\n",
    "    \n",
    "    # Tile out pixels (if we have 4 filters arrange as below), spaced out\n",
    "    # [1 2 1 2 ...]\n",
    "    # [3 4 3 4 ...]\n",
    "    # [...]\n",
    "    x = tf.layers.conv2d(x, c * scale ** 2, (3, 3), activation=tf.nn.relu, padding='same')\n",
    "    # (3,3) is stride\n",
    "    \n",
    "    # This does the rearrangement\n",
    "    output = tf.depth_to_space(x, scale)\n",
    "    return output\n",
    "    \n",
    "tf.reset_default_graph()\n",
    "x = tf.placeholder(tf.float32, shape=[None, 32, 32, 3])\n",
    "x_up = upscale_block(x)\n",
    "print(x_up)\n",
    "\n",
    "# Note we get many more parameters because here we have 4 times as many channels as we had in\n",
    "num_params = np.sum([np.prod(v.get_shape().as_list()) for v in tf.trainable_variables()])\n",
    "print('Parameters: ' + str(num_params))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ProgressiveGAN upscaling\n",
    "\n",
    "Finally, one technique that's recently found massive success in Nvidia's ProgressiveGAN used to generate high-resolution fake celebrity faces:\n",
    "\n",
    "<img src=\"https://i2.wp.com/robotnyheter.se/wp-content/uploads/2018/01/Nvidia_GAN_ansikten.jpg?w=1561\" width=\"70%\">\n",
    "\n",
    "None of these are real photos, they've all been upsampled from Gaussian noise with this technique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"conv2d/Relu:0\", shape=(?, 64, 64, 3), dtype=float32)\n",
      "Parameters: 84\n"
     ]
    }
   ],
   "source": [
    "def upscale_block(x, scale=2):\n",
    "    \"\"\" similar to the upsampling used in [ProgressiveGAN](https://arxiv.org/pdf/1710.10196.pdf) \"\"\"\n",
    "    n, w, h, c = x.get_shape().as_list()\n",
    "    # This is a bilinear upscale (older algorithm)\n",
    "    # Problem is you of course get a lot of fuzziness in images\n",
    "    up_x = tf.image.resize_nearest_neighbor(x, [scale*h, scale*w])\n",
    "    # Here the convolution is used to sharpen it\n",
    "    output = tf.layers.conv2d(up_x, 3, 3, padding='same', activation=tf.nn.relu)\n",
    "    return output\n",
    "\n",
    "tf.reset_default_graph()\n",
    "x = tf.placeholder(tf.float32, shape=[None, 32, 32, 3])\n",
    "x_up = upscale_block(x)\n",
    "print(x_up)\n",
    "\n",
    "# Note here we are back down to 84 parameters as we only kept 3 channels without creating new ones\n",
    "num_params = np.sum([np.prod(v.get_shape().as_list()) for v in tf.trainable_variables()])\n",
    "print('Parameters: ' + str(num_params))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autoencoding\n",
    "\n",
    "Generally, autoencoding is learning \"a complicated identity function\". This makes it a form of unsupervised learning, which doesn't require data to be explicitly labeled, but instead looks for patterns and trends in data. Typically the complication is to bottleneck the size of the representation, but can also be more varied. We'll look at code for sparse autoencoders and de-noising autoencoders.\n",
    "\n",
    "First we'll define some preliminaries that we'll use in both architectures:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def upscale_block(x, scale=2):\n",
    "    \"\"\"transpose convolution upscale\"\"\"\n",
    "    # Here we upscale by factor of 2\n",
    "    return tf.layers.conv2d_transpose(x, 1, 3, strides=(scale, scale), padding='same', activation=tf.nn.relu)\n",
    "\n",
    "# use convolution for downscaling\n",
    "def downscale_block(x, scale=2):\n",
    "    n, h, w, c = x.get_shape().as_list()\n",
    "    # Here we downscale by factor of 2\n",
    "    return tf.layers.conv2d(x, np.floor(c * 1.25), 3, strides=scale, padding='same')\n",
    "\n",
    "def autoencoder_network(x, code_size=100):\n",
    "    \"\"\"This network assumes [?, 28, 28, ?] shaped input\"\"\"\n",
    "    # encoder\n",
    "    encoder_14 = downscale_block(x)\n",
    "    encoder_7 = downscale_block(encoder_14)\n",
    "    flatten_dim = np.prod(encoder_7.get_shape().as_list()[1:])\n",
    "    flat = tf.reshape(encoder_7, [-1, flatten_dim])\n",
    "    # code\n",
    "    code = tf.layers.dense(flat, code_size, activation=tf.nn.relu)\n",
    "    # decoder\n",
    "    # 49 neurons used as I need to end up with 28x28x1 output (not entirely sure...)\n",
    "    # 28 = (7+7)*2\n",
    "    hidden_decoder = tf.layers.dense(code, 49, activation=tf.nn.elu)\n",
    "    decoder_7 = tf.reshape(hidden_decoder, [-1, 7, 7, 1])\n",
    "    decoder_14 = upscale_block(decoder_7)\n",
    "    output = upscale_block(decoder_14)\n",
    "    return code, output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sparse autoenoding\n",
    "\n",
    "Although we bottleneck the representation in normal autoencoding by reducing the dimensionality, sparse autoencoders can actually increase it, but restrict it to be sparsely activated with L1 regularization using [tf.norm](https://www.tensorflow.org/api_docs/python/tf/norm) or KL-divergence. This has the effect of only having non-zero values in a few dimensions, effectively bottlenecking each representation, but giving a greater variety of dimensions to choose to be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1012 12:20:13.983966 47958307618944 deprecation.py:323] From <ipython-input-6-6675f071ffbc>:20: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dense instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters: 9989\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# This uses L1 regularizer penalizes for every non-zero vector\n",
    "# This means we want to use as few indices in vector as possible\n",
    "\n",
    "# It does not bottleneck amount of data that can pass through though, which the standard auto-encoder does\n",
    "\n",
    "# set hyperparameters\n",
    "sparsity_weight = 5e-4 # 5e-3 made output all zero\n",
    "code_size = 100\n",
    "EPOCHS = 1\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# define graph\n",
    "tf.reset_default_graph()\n",
    "x = tf.placeholder(tf.float32, shape=[None, 28, 28, 1])\n",
    "# Defined in autoencoder_network above\n",
    "code, outputs = autoencoder_network(x, code_size)\n",
    "# Code would be the output of the autoencoder if we were doing transfer learning, as this is most compressed\n",
    "\n",
    "# just for fun count number of parameters in network\n",
    "num_params = np.sum([np.prod(v.get_shape().as_list()) for v in tf.trainable_variables()])\n",
    "print('Parameters: ' + str(num_params))\n",
    "\n",
    "# calculate loss\n",
    "sparsity_loss = tf.norm(code, ord=1, axis=1) # Calculates L1 norm\n",
    "reconstruction_loss = tf.reduce_mean(tf.square(outputs - x)) # Mean Square Error\n",
    "total_loss = reconstruction_loss + sparsity_weight * sparsity_loss # total loss including both\n",
    "\n",
    "# setup optimizer\n",
    "optimizer = tf.train.AdamOptimizer()\n",
    "train_op = optimizer.minimize(total_loss)\n",
    "\n",
    "# train for an epoch\n",
    "session = tf.Session()\n",
    "session.run(tf.global_variables_initializer())\n",
    "for epoch in range(EPOCHS):\n",
    "    for i in range(train_data.shape[0] // BATCH_SIZE):\n",
    "        batch_xs = train_data[i*BATCH_SIZE:(i+1)*BATCH_SIZE, :]\n",
    "        session.run(train_op, {x: batch_xs})\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Code and Input\n",
      "[[-0.          0.1010787  -0.         -0.          0.15245928 -0.\n",
      "   0.930921    0.4564608  -0.         -0.         -0.         -0.\n",
      "  -0.         -0.          0.16466914  0.1844     -0.         -0.\n",
      "  -0.         -0.         -0.          0.5640377  -0.         -0.\n",
      "  -0.         -0.         -0.         -0.         -0.          0.18913792\n",
      "  -0.          0.5015242  -0.         -0.         -0.          0.23799577\n",
      "   0.9350324  -0.         -0.         -0.         -0.         -0.\n",
      "  -0.         -0.         -0.          0.37871265  0.41970932 -0.\n",
      "   0.00424195  0.22550482 -0.         -0.         -0.         -0.\n",
      "  -0.         -0.         -0.         -0.          0.88969725  0.700522\n",
      "  -0.         -0.         -0.          0.28670785  0.63554966  0.7244459\n",
      "   0.5065285  -0.         -0.         -0.          0.1729887  -0.\n",
      "  -0.         -0.         -0.          0.35234573 -0.         -0.\n",
      "  -0.         -0.         -0.         -0.         -0.         -0.\n",
      "  -0.         -0.         -0.         -0.         -0.         -0.\n",
      "  -0.         -0.         -0.         -0.         -0.         -0.\n",
      "  -0.         -0.         -0.         -0.        ]]\n",
      "Number of nonzero code dimensions: 23/100\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Now run a test\n",
    "idx = np.random.randint(test_data.shape[0])\n",
    "# Run chosen data through autoencoder\n",
    "x_out, code_out, output_out = session.run([x, code, outputs], {x: np.expand_dims(test_data[idx], axis=0)})\n",
    "\n",
    "# Visualize code and output\n",
    "print(\"Code and Input\")\n",
    "plt.imshow(np.squeeze(x_out))\n",
    "print(code_out)\n",
    "print(\"Number of nonzero code dimensions: {}/{}\".format(np.count_nonzero(code_out), code_size))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstruction\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x2b9e9514bb38>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Show reconstruction\n",
    "\n",
    "# If reconstruction is terrible check code\n",
    "# If it is all zeros then you have too high of a constraint on the L1 error\n",
    "# Lowering this may help\n",
    "\n",
    "# This is in a different cell from the last plt.imshow call to show 2 images at once\n",
    "print(\"Reconstruction\")\n",
    "plt.imshow(np.squeeze(output_out))\n",
    "\n",
    "# Very blocky as transfpose is not very great at resolution\n",
    "# Using sharpening approches will likely provide better results in terms of resolution\n",
    "\n",
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Denoising Autoencoder\n",
    "\n",
    "Another way to force the an autoencoder to learn the features of data is to force it to map noisy, corrupted versions of the data back to the original. This is usually accomplished by manually adding noice (e.g., Gaussian), but may also be useful in real world settings.\n",
    "\n",
    "The noise level could be scaled up as training proceeds to implement a form of [curriculum learning](https://ronan.collobert.com/pub/matos/2009_curriculum_icml.pdf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# Works fundamentally different than sparse autoencoder\n",
    "# Take clean image and add noise to it to distort it\n",
    "# Then run through autoencoder and pass in distorted version and ask for original image\n",
    "# It thus needs to learn to separate signal from noise\n",
    "\n",
    "# hyperparameters\n",
    "noise_level = 0.1\n",
    "code_size = 40\n",
    "\n",
    "# define graph\n",
    "tf.reset_default_graph()\n",
    "x = tf.placeholder(tf.float32, shape=[None, 28, 28, 1])\n",
    "# Add gaussian noise on top of input\n",
    "x_noisy = x + noise_level * tf.random_normal(tf.shape(x))\n",
    "# Pass noisy image into network\n",
    "code, outputs = autoencoder_network(x_noisy, code_size=code_size)\n",
    "\n",
    "# calculate loss\n",
    "reconstruction_loss = tf.reduce_mean(tf.square(outputs - x)) # MSE\n",
    "total_loss = reconstruction_loss # just for consistency even though there is only one term\n",
    "\n",
    "# setup optimizer\n",
    "optimizer = tf.train.AdamOptimizer()\n",
    "train_op = optimizer.minimize(total_loss)\n",
    "\n",
    "# train for an epoch and visualize\n",
    "batch_size = 16\n",
    "session = tf.Session()\n",
    "session.run(tf.global_variables_initializer())\n",
    "for epoch in range(2):\n",
    "    for i in range(train_data.shape[0] // batch_size):\n",
    "        batch_xs = train_data[i*batch_size:(i+1)*batch_size, :]\n",
    "        session.run(train_op, {x: batch_xs})\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x2b9ec09977f0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# run a test\n",
    "idx = np.random.randint(test_data.shape[0])\n",
    "x_out, noisy_x_out, code_out, output_out = session.run([x, x_noisy, code, outputs], {x: np.expand_dims(test_data[idx], axis=0)})\n",
    "print(\"Input\")\n",
    "plt.imshow(np.squeeze(x_out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Code and Noised Input\n",
      "[[ 0.6735974   0.9724092   1.1182804   0.7309284   0.2113837   0.7636273\n",
      "   1.1857438   1.394796    1.0736072   1.0191958  -0.          0.4363351\n",
      "  -0.          0.56239116  0.6878202   0.5134084   0.7393745   1.3263414\n",
      "   1.3831631   1.1395414   0.92428166 -0.          0.81724614  1.335417\n",
      "   0.6488855   1.2148685   0.8766148   0.50705504  1.1391382   0.85937315\n",
      "  -0.          0.15663268  0.9172243   0.6905645  -0.          1.4398689\n",
      "   0.08250363  1.2056785   1.1510267   0.75959617]]\n",
      "Number of nonzero code dimensions: 35/40\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# show the noised input and code\n",
    "plt.imshow(np.squeeze(noisy_x_out))\n",
    "print(\"Code and Noised Input\")\n",
    "print(code_out)\n",
    "print(\"Number of nonzero code dimensions: {}/{}\".format(np.count_nonzero(code_out), code_size))\n",
    "\n",
    "# Many less zeros in code as there is no restriction to specifically push them to zero\n",
    "# If you add L1 regularization it may help, but it could be hard to balance hyperparameters with noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstruction\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x2b9ec0a50630>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Show reconstruction\n",
    "# This has to be in a different cell from the last plt.imshow call to show 2 images at once\n",
    "print(\"Reconstruction\")\n",
    "plt.imshow(np.squeeze(output_out))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write code specifying an autoencoder with a code of shape `[None, 4, 4, 1]` with the provided input placeholder (corresponding to a color image like CIFAR-10). Specify a reasonable autoencoding loss function so that the given optimizer code runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't need to train this\n",
    "# Just create valid autoencoder so the lines below run correctly\n",
    "\n",
    "tf.reset_default_graph()\n",
    "x = tf.placeholder(tf.float32, shape=[None, 32, 32, 3])\n",
    "\n",
    "# your code here\n",
    "\n",
    "# Cut down to 16x16\n",
    "encoder_1 = tf.compat.v1.layers.Conv2D(64, (5,5), 2, padding='same', name='encode_1', kernel_regularizer=tf.contrib.layers.l2_regularizer(0.1))(x)\n",
    "# Cut down to 8x8\n",
    "encoder_2 = tf.compat.v1.layers.Conv2D(32, (3,3), 2, padding='same', name='encode_2', kernel_regularizer=tf.contrib.layers.l2_regularizer(0.1))(encoder_1)\n",
    "# Cut down to 4x4\n",
    "encoder_3 = tf.compat.v1.layers.Conv2D(1, (3,3), 2, padding='same', name='encode_3', kernel_regularizer=tf.contrib.layers.l2_regularizer(0.1))(encoder_2)\n",
    "# Flatten for use in dense layer code\n",
    "flat = tf.reshape(encoder_3, [-1, 4,4,1], name='flat')\n",
    "# Add code layer (16 neurons total from 4x4x1)\n",
    "code = tf.compat.v1.layers.Dense( 16, activation=tf.nn.relu, name='code')(flat)\n",
    "# Reshape back for convolutional layers\n",
    "reshape_layer = tf.reshape(code, [-1, 4, 4, 1], name='decode_1')\n",
    "# Upscale to 8x8\n",
    "decoder_1 = tf.compat.v1.layers.Conv2DTranspose(1, (3,3), 2, padding='same', name='decode_1')(reshape_layer) \n",
    "# Upscale to 16x16\n",
    "decoder_2 = tf.compat.v1.layers.Conv2DTranspose(32, (3,3), 2, padding='same', name='decode_2')(decoder_1)\n",
    "# Upscale to 32x32\n",
    "decoder_3 = tf.compat.v1.layers.Conv2DTranspose(64, (5,5), 2, padding='same', name='decode_3')(decoder_2)\n",
    "# Calculate output\n",
    "output = tf.compat.v1.layers.Conv2DTranspose(3, (3,3), 1, padding='same', name='output')(decoder_3)\n",
    "\n",
    "\n",
    "# calculate loss\n",
    "sparsity_weight = 5e-3\n",
    "sparsity_loss = tf.norm(code, ord=1, axis=1)\n",
    "reconstruction_loss = tf.reduce_mean(tf.square(output - x)) # Mean Square Error\n",
    "loss = reconstruction_loss + sparsity_weight * sparsity_loss\n",
    "\n",
    "\n",
    "# if your code works, this should have no problem\n",
    "optimizer = tf.train.AdamOptimizer()\n",
    "train_op = optimizer.minimize(loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coda\n",
    "\n",
    "#### [Progressive GAN latent space interpolation on Youtube](https://youtu.be/XOxxPcy5Gr4?t=1m48s)\n",
    "\n",
    "#### [How to Use t-SNE Effectively (distill.pub)](https://distill.pub/2016/misread-tsne/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
